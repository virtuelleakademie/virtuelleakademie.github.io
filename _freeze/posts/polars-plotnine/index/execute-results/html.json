{
  "hash": "f31dc0b4f3026fed8e60fa807d7a57a2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lightning-Fast Data Analysis: Polars and plotnine for Modern Python\"\ndate: 6 June, 2025\ndate-format: \"DD MMM, YYYY\"\n\nauthor: \n  - name: Andrew Ellis\n    url: https://github.com/awellis\n    affiliation: Virtual Academy, Bern University of Applied Sciences\n    affiliation-url: https://virtuelleakademie.ch\n    orcid: 0000-0002-2788-936X\n\ncategories: [Python, Polars, plotnine, data manipulation, visualization, performance, tutorial]\nformat:\n    html:\n        code-fold: true\n        code-tools: true\n        code-summary: \"Show the code\"\n        toc: true\n---\n\nThis tutorial explores the powerful combination of **Polars** and **plotnine** for high-performance data analysis in Python. Polars brings lightning-fast data manipulation with lazy evaluation, while plotnine provides ggplot2's elegant grammar of graphics for Python, creating a modern alternative to the pandas + matplotlib/seaborn ecosystem.\n\n## Why Polars + plotnine?\n\n### The Performance Revolution: Polars\n\nPolars is a blazingly fast DataFrame library that leverages:\n\n- **Rust backend**: Memory-efficient and CPU-optimized operations\n- **Lazy evaluation**: Query optimization before execution  \n- **Columnar processing**: Apache Arrow format for speed\n- **Parallel execution**: Automatic multi-threading\n- **Expressive API**: Clean, readable data manipulation syntax\n\n### The Grammar Advantage: plotnine\n\nplotnine brings ggplot2's grammar of graphics to Python:\n\n- **Declarative syntax**: Describe what you want, not how to draw it\n- **Layered approach**: Build complex plots incrementally  \n- **Consistent aesthetics**: Systematic approach to visual mapping\n- **Extensible**: Easy customization and theming\n- **Educational**: Matches R's ggplot2 for cross-language consistency\n\n## Setup and Data Preparation\n\n::: {#d455fe85 .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport polars as pl\nimport numpy as np\nfrom plotnine import (\n    ggplot, aes, geom_point, geom_smooth, geom_violin, geom_boxplot, geom_col, \n    geom_tile, geom_text, stat_summary, facet_wrap,\n    scale_color_brewer, scale_fill_brewer, scale_color_manual, scale_fill_manual,\n    scale_color_gradient, scale_fill_gradient, scale_color_gradient2, \n    scale_fill_gradient2, scale_size_continuous, scale_alpha_continuous,\n    labs, theme_minimal, theme, element_text, element_rect, element_line,\n    element_blank, guide_legend, coord_flip, xlim, ylim, position_dodge\n)\nimport plotnine.options\nfrom datetime import datetime, timedelta\nimport warnings\n\n# Configure plotnine for better output\nplotnine.options.figure_size = (10, 6)\nplotnine.options.dpi = 100\nwarnings.filterwarnings('ignore')\n\n# Ensure proper display in Quarto\nfrom IPython.display import display\nimport matplotlib\nmatplotlib.use('Agg')  # Use non-interactive backend\n\n# Display Polars version and configuration\nprint(f\"Polars version: {pl.__version__}\")\nprint(f\"Available threads: {pl.thread_pool_size()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPolars version: 1.30.0\nAvailable threads: 14\n```\n:::\n:::\n\n\n::: {#d8ffbfc7 .cell execution_count=2}\n``` {.python .cell-code}\n# Create a comprehensive educational dataset using Polars\n# This demonstrates Polars' syntax while creating realistic data\n\nnp.random.seed(42)\n\n# Generate base student data\nn_students = 5000\nn_courses = 8\nn_semesters = 6\n\n# Create students DataFrame\nstudents_df = pl.DataFrame({\n    \"student_id\": range(1, n_students + 1),\n    \"age\": np.random.normal(22, 2, n_students).round().astype(int),\n    \"program\": np.random.choice([\"Computer Science\", \"Mathematics\", \"Physics\", \"Statistics\"], n_students),\n    \"entry_year\": np.random.choice([2020, 2021, 2022, 2023], n_students),\n    \"study_mode\": np.random.choice([\"Full-time\", \"Part-time\"], n_students, p=[0.8, 0.2])\n}).with_columns([\n    # Add realistic constraints using Polars expressions\n    pl.col(\"age\").clip(18, 28).alias(\"age\"),\n    # Generate GPA with program-based bias\n    pl.when(pl.col(\"program\") == \"Computer Science\")\n      .then(np.random.normal(3.2, 0.5, n_students))\n      .when(pl.col(\"program\") == \"Mathematics\") \n      .then(np.random.normal(3.4, 0.4, n_students))\n      .when(pl.col(\"program\") == \"Physics\")\n      .then(np.random.normal(3.1, 0.6, n_students))\n      .otherwise(np.random.normal(3.3, 0.5, n_students))\n      .clip(1.0, 4.0)\n      .round(2)\n      .alias(\"gpa\")\n])\n\nprint(\"Students DataFrame shape:\", students_df.shape)\nstudents_df.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStudents DataFrame shape: (5000, 6)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>student_id</th><th>age</th><th>program</th><th>entry_year</th><th>study_mode</th><th>gpa</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>1</td><td>23</td><td>&quot;Statistics&quot;</td><td>2022</td><td>&quot;Full-time&quot;</td><td>2.43</td></tr><tr><td>2</td><td>22</td><td>&quot;Computer Science&quot;</td><td>2020</td><td>&quot;Full-time&quot;</td><td>2.54</td></tr><tr><td>3</td><td>23</td><td>&quot;Statistics&quot;</td><td>2020</td><td>&quot;Part-time&quot;</td><td>2.88</td></tr><tr><td>4</td><td>25</td><td>&quot;Mathematics&quot;</td><td>2023</td><td>&quot;Full-time&quot;</td><td>3.58</td></tr><tr><td>5</td><td>22</td><td>&quot;Statistics&quot;</td><td>2021</td><td>&quot;Full-time&quot;</td><td>3.94</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n::: {#7f626154 .cell execution_count=3}\n``` {.python .cell-code}\n# Create course performance data using a simple approach\ncourses = [\"Calculus\", \"Linear Algebra\", \"Statistics\", \"Programming\", \n          \"Data Structures\", \"Machine Learning\", \"Research Methods\", \"Thesis\"]\n\n# Create performance data manually to avoid cross join issues\nnp.random.seed(42)\nperformance_records = []\n\n# Get student data as list for iteration\nstudent_records = students_df.to_dicts()\n\nfor student in student_records:\n    for i, course in enumerate(courses):\n        # Course difficulty multipliers\n        if course in [\"Machine Learning\", \"Thesis\"]:\n            base_multiplier = 20\n            noise_factor = 1.6\n        elif course in [\"Calculus\", \"Linear Algebra\"]:\n            base_multiplier = 22\n            noise_factor = 2.0\n        else:\n            base_multiplier = 21\n            noise_factor = 1.2\n        \n        # Generate pseudo-random values based on student_id and course\n        seed_val = (student[\"student_id\"] * 7 + i * 13) % 1000\n        \n        # Calculate score\n        base_score = student[\"gpa\"] * base_multiplier\n        score_variation = (seed_val / 100.0 - 5.0) * noise_factor\n        score = max(0, min(100, round(base_score + score_variation, 1)))\n        \n        # Study hours based on course type\n        if course in [\"Programming\", \"Data Structures\"]:\n            study_hours = 8 + (seed_val % 100) / 10.0\n        elif course == \"Thesis\":\n            study_hours = 15 + (seed_val % 150) / 10.0\n        else:\n            study_hours = 5 + (seed_val % 80) / 10.0\n        \n        # Attendance\n        attendance = max(50, min(100, round(85 + (seed_val % 50) / 5.0 - 5.0, 1)))\n        \n        performance_records.append({\n            \"student_id\": student[\"student_id\"],\n            \"program\": student[\"program\"],\n            \"gpa\": student[\"gpa\"],\n            \"course\": course,\n            \"semester\": i + 1,\n            \"score\": score,\n            \"study_hours\": round(study_hours, 1),\n            \"attendance\": attendance\n        })\n\n# Create Polars DataFrame from the records\nperformance_df = pl.DataFrame(performance_records)\n\nprint(\"Performance DataFrame shape:\", performance_df.shape)\nperformance_df.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPerformance DataFrame shape: (40000, 8)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>student_id</th><th>program</th><th>gpa</th><th>course</th><th>semester</th><th>score</th><th>study_hours</th><th>attendance</th></tr><tr><td>i64</td><td>str</td><td>f64</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1</td><td>&quot;Statistics&quot;</td><td>2.43</td><td>&quot;Calculus&quot;</td><td>1</td><td>43.6</td><td>5.7</td><td>81.4</td></tr><tr><td>1</td><td>&quot;Statistics&quot;</td><td>2.43</td><td>&quot;Linear Algebra&quot;</td><td>2</td><td>43.9</td><td>7.0</td><td>84.0</td></tr><tr><td>1</td><td>&quot;Statistics&quot;</td><td>2.43</td><td>&quot;Statistics&quot;</td><td>3</td><td>45.4</td><td>8.3</td><td>86.6</td></tr><tr><td>1</td><td>&quot;Statistics&quot;</td><td>2.43</td><td>&quot;Programming&quot;</td><td>4</td><td>45.6</td><td>12.6</td><td>89.2</td></tr><tr><td>1</td><td>&quot;Statistics&quot;</td><td>2.43</td><td>&quot;Data Structures&quot;</td><td>5</td><td>45.7</td><td>13.9</td><td>81.8</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n## Polars Data Manipulation Mastery\n\n### 1. Basic Operations and Lazy Evaluation\n\n::: {#69175938 .cell execution_count=4}\n``` {.python .cell-code}\n# Demonstrate Polars' lazy evaluation\nlazy_query = (\n    performance_df\n    .lazy()  # Switch to lazy mode\n    .filter(pl.col(\"score\") >= 70)\n    .group_by([\"program\", \"course\"])\n    .agg([\n        pl.col(\"score\").mean().alias(\"avg_score\"),\n        pl.col(\"study_hours\").mean().alias(\"avg_study_hours\"),\n        pl.col(\"attendance\").mean().alias(\"avg_attendance\"),\n        pl.count().alias(\"n_students\")\n    ])\n    .sort(\"avg_score\", descending=True)\n)\n\n# Execute the lazy query\nprogram_performance = lazy_query.collect()\nprint(\"Top performing program-course combinations:\")\nprogram_performance.head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop performing program-course combinations:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (10, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>program</th><th>course</th><th>avg_score</th><th>avg_study_hours</th><th>avg_attendance</th><th>n_students</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td></tr></thead><tbody><tr><td>&quot;Mathematics&quot;</td><td>&quot;Calculus&quot;</td><td>80.371966</td><td>8.828641</td><td>84.946117</td><td>824</td></tr><tr><td>&quot;Mathematics&quot;</td><td>&quot;Linear Algebra&quot;</td><td>80.352906</td><td>8.866828</td><td>84.859564</td><td>826</td></tr><tr><td>&quot;Statistics&quot;</td><td>&quot;Calculus&quot;</td><td>80.228754</td><td>8.865864</td><td>84.898867</td><td>706</td></tr><tr><td>&quot;Statistics&quot;</td><td>&quot;Linear Algebra&quot;</td><td>80.203841</td><td>8.831721</td><td>84.912376</td><td>703</td></tr><tr><td>&quot;Physics&quot;</td><td>&quot;Calculus&quot;</td><td>79.930411</td><td>9.084794</td><td>85.010376</td><td>559</td></tr><tr><td>&quot;Physics&quot;</td><td>&quot;Linear Algebra&quot;</td><td>79.864298</td><td>8.949378</td><td>84.705151</td><td>563</td></tr><tr><td>&quot;Computer Science&quot;</td><td>&quot;Linear Algebra&quot;</td><td>79.621408</td><td>8.850733</td><td>84.927273</td><td>682</td></tr><tr><td>&quot;Computer Science&quot;</td><td>&quot;Calculus&quot;</td><td>79.557143</td><td>8.905102</td><td>85.037609</td><td>686</td></tr><tr><td>&quot;Physics&quot;</td><td>&quot;Programming&quot;</td><td>77.858796</td><td>13.072222</td><td>85.144444</td><td>432</td></tr><tr><td>&quot;Physics&quot;</td><td>&quot;Statistics&quot;</td><td>77.850229</td><td>8.884404</td><td>85.030275</td><td>436</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n::: {#286fe6fe .cell execution_count=5}\n``` {.python .cell-code}\n# Advanced Polars expressions and window functions\nstudent_rankings = (\n    performance_df\n    .with_columns([\n        # Calculate percentile rank within each course\n        pl.col(\"score\").rank(method=\"average\").over(\"course\").alias(\"course_rank\"),\n        \n        # Calculate student average score\n        pl.col(\"score\").mean().over(\"student_id\").alias(\"student_avg\"),\n        \n        # Flag high performers (top 10% in course) - simplified calculation\n        (pl.col(\"score\").rank(method=\"average\", descending=True).over(\"course\") <= \n         (pl.col(\"score\").count().over(\"course\") * 0.1).cast(pl.Int64)).alias(\"top_performer\")\n    ])\n    .filter(pl.col(\"semester\") >= 4)  # Focus on advanced courses\n)\n\nprint(\"Student rankings with advanced metrics:\")\nstudent_rankings.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStudent rankings with advanced metrics:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>student_id</th><th>program</th><th>gpa</th><th>course</th><th>semester</th><th>score</th><th>study_hours</th><th>attendance</th><th>course_rank</th><th>student_avg</th><th>top_performer</th></tr><tr><td>i64</td><td>str</td><td>f64</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>bool</td></tr></thead><tbody><tr><td>1</td><td>&quot;Statistics&quot;</td><td>2.43</td><td>&quot;Programming&quot;</td><td>4</td><td>45.6</td><td>12.6</td><td>89.2</td><td>137.0</td><td>44.275</td><td>false</td></tr><tr><td>1</td><td>&quot;Statistics&quot;</td><td>2.43</td><td>&quot;Data Structures&quot;</td><td>5</td><td>45.7</td><td>13.9</td><td>81.8</td><td>136.5</td><td>44.275</td><td>false</td></tr><tr><td>1</td><td>&quot;Statistics&quot;</td><td>2.43</td><td>&quot;Machine Learning&quot;</td><td>6</td><td>41.8</td><td>12.2</td><td>84.4</td><td>115.0</td><td>44.275</td><td>false</td></tr><tr><td>1</td><td>&quot;Statistics&quot;</td><td>2.43</td><td>&quot;Research Methods&quot;</td><td>7</td><td>46.0</td><td>5.5</td><td>87.0</td><td>143.5</td><td>44.275</td><td>false</td></tr><tr><td>1</td><td>&quot;Statistics&quot;</td><td>2.43</td><td>&quot;Thesis&quot;</td><td>8</td><td>42.2</td><td>24.8</td><td>89.6</td><td>124.0</td><td>44.275</td><td>false</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n### 2. Complex Aggregations and Transformations\n\n::: {#a3ed0ee1 .cell execution_count=6}\n``` {.python .cell-code}\n# Multi-level aggregations using Polars\nprogram_analysis = (\n    student_rankings\n    .group_by(\"program\")\n    .agg([\n        # Basic statistics\n        pl.col(\"score\").mean().alias(\"avg_score\"),\n        pl.col(\"score\").std().alias(\"std_score\"),\n        pl.col(\"score\").quantile(0.5).alias(\"median_score\"),\n        \n        # Advanced metrics\n        pl.col(\"top_performer\").sum().alias(\"top_performers_count\"),\n        pl.col(\"top_performer\").mean().alias(\"top_performer_rate\"),\n        \n        # Study behavior\n        pl.col(\"study_hours\").mean().alias(\"avg_study_hours\"),\n        pl.col(\"attendance\").mean().alias(\"avg_attendance\"),\n        \n        # Count and range\n        pl.count().alias(\"total_records\"),\n        (pl.col(\"score\").max() - pl.col(\"score\").min()).alias(\"score_range\")\n    ])\n    .sort(\"avg_score\", descending=True)\n)\n\nprint(\"Comprehensive program analysis:\")\nprogram_analysis\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComprehensive program analysis:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>program</th><th>avg_score</th><th>std_score</th><th>median_score</th><th>top_performers_count</th><th>top_performer_rate</th><th>avg_study_hours</th><th>avg_attendance</th><th>total_records</th><th>score_range</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Mathematics&quot;</td><td>70.218913</td><td>8.735432</td><td>70.4</td><td>737</td><td>0.123244</td><td>13.215217</td><td>84.922074</td><td>5980</td><td>50.5</td></tr><tr><td>&quot;Statistics&quot;</td><td>67.450602</td><td>10.359923</td><td>67.7</td><td>700</td><td>0.11245</td><td>13.102892</td><td>84.893012</td><td>6225</td><td>62.2</td></tr><tr><td>&quot;Computer Science&quot;</td><td>65.771991</td><td>10.61858</td><td>66.0</td><td>512</td><td>0.078108</td><td>13.151869</td><td>84.879786</td><td>6555</td><td>62.8</td></tr><tr><td>&quot;Physics&quot;</td><td>63.37617</td><td>12.308192</td><td>63.5</td><td>536</td><td>0.085897</td><td>13.204647</td><td>84.907051</td><td>6240</td><td>74.3</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n::: {#ea854684 .cell execution_count=7}\n``` {.python .cell-code}\n# For correlation analysis, we'll use a simpler approach\n# Calculate correlations using pandas (since plotnine uses pandas anyway)\nimport pandas as pd\n\ncorrelation_results = []\nfor program in performance_df[\"program\"].unique():\n    program_data = performance_df.filter(pl.col(\"program\") == program).to_pandas()\n    \n    score_study_corr = program_data[\"score\"].corr(program_data[\"study_hours\"])\n    score_attendance_corr = program_data[\"score\"].corr(program_data[\"attendance\"])\n    \n    correlation_results.append({\n        \"program\": program,\n        \"score_study_correlation\": round(score_study_corr, 3),\n        \"score_attendance_correlation\": round(score_attendance_corr, 3)\n    })\n\ncorrelation_df = pl.DataFrame(correlation_results)\n\n# Combine with program analysis\nfinal_program_analysis = program_analysis.join(correlation_df, on=\"program\")\nprint(\"\\nProgram analysis with correlations:\")\nfinal_program_analysis\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nProgram analysis with correlations:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>program</th><th>avg_score</th><th>std_score</th><th>median_score</th><th>top_performers_count</th><th>top_performer_rate</th><th>avg_study_hours</th><th>avg_attendance</th><th>total_records</th><th>score_range</th><th>score_study_correlation</th><th>score_attendance_correlation</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Mathematics&quot;</td><td>70.218913</td><td>8.735432</td><td>70.4</td><td>737</td><td>0.123244</td><td>13.215217</td><td>84.922074</td><td>5980</td><td>50.5</td><td>-0.113</td><td>0.019</td></tr><tr><td>&quot;Computer Science&quot;</td><td>65.771991</td><td>10.61858</td><td>66.0</td><td>512</td><td>0.078108</td><td>13.151869</td><td>84.879786</td><td>6555</td><td>62.8</td><td>-0.08</td><td>0.025</td></tr><tr><td>&quot;Physics&quot;</td><td>63.37617</td><td>12.308192</td><td>63.5</td><td>536</td><td>0.085897</td><td>13.204647</td><td>84.907051</td><td>6240</td><td>74.3</td><td>-0.063</td><td>0.014</td></tr><tr><td>&quot;Statistics&quot;</td><td>67.450602</td><td>10.359923</td><td>67.7</td><td>700</td><td>0.11245</td><td>13.102892</td><td>84.893012</td><td>6225</td><td>62.2</td><td>-0.094</td><td>0.021</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n## Declarative Visualization with plotnine\n\n### 3. Grammar of Graphics Implementation\n\n::: {#131e339b .cell fig-height='8' fig-width='12' execution_count=8}\n``` {.python .cell-code}\n# Convert Polars to pandas for plotnine (plotnine expects pandas)\nperformance_pd = performance_df.to_pandas()\n\n# Configure plotnine for this specific plot\nimport plotnine.options\nplotnine.options.figure_size = (12, 8)\n\n# Create a sophisticated multi-faceted visualization\np1 = (\n    ggplot(performance_pd, aes(x=\"study_hours\", y=\"score\", color=\"program\")) +\n    geom_point(alpha=0.6, size=1.5) +\n    geom_smooth(method=\"lm\", se=True, size=1.2) +\n    facet_wrap(\"course\", ncol=4, scales=\"free\") +\n    scale_color_brewer(type=\"qual\", palette=\"Set2\") +\n    labs(\n        title=\"Relationship Between Study Hours and Academic Performance\",\n        subtitle=\"Linear trends with 95% confidence intervals across courses and programs\",\n        x=\"Weekly Study Hours\",\n        y=\"Course Score (%)\",\n        color=\"Academic Program\",\n        caption=\"Data: Simulated student performance (n=5,000 students, 8 courses)\"\n    ) +\n    theme_minimal() +\n    theme(\n        plot_title=element_text(size=14, weight=\"bold\"),\n        plot_subtitle=element_text(size=11, color=\"#666666\"),\n        strip_text=element_text(size=10, weight=\"bold\"),\n        legend_position=\"bottom\"\n    )\n)\n\n\n# Display the plot\np1\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=1200 height=800}\n:::\n:::\n\n\n### 4. Advanced Layered Visualizations\n\n::: {#05757d67 .cell fig-height='6' fig-width='10' execution_count=9}\n``` {.python .cell-code}\n# Configure plotnine for this plot\nplotnine.options.figure_size = (10, 6)\n\n# Aggregate data for program comparison\nprogram_summary = program_analysis.to_pandas()\n\n# Create a sophisticated comparison plot\np2 = (\n    ggplot(program_summary, aes(x=\"avg_study_hours\", y=\"avg_score\")) +\n    \n    # Add confidence ellipses based on standard deviation\n    geom_point(aes(size=\"total_records\", color=\"top_performer_rate\"), alpha=0.8) +\n    \n    # Add program labels\n    geom_text(aes(label=\"program\"), nudge_y=1.5, size=9, fontweight=\"bold\") +\n    \n    # Add trend line\n    geom_smooth(method=\"lm\", color=\"darkred\", linetype=\"dashed\", se=False) +\n    \n    # Customize scales\n    scale_size_continuous(\n        name=\"Total Records\",\n        range=(8, 15),\n        guide=guide_legend(override_aes={\"alpha\": 1})\n    ) +\n    scale_color_gradient2(\n        name=\"Top Performer\\nRate\",\n        low=\"blue\", mid=\"white\", high=\"red\",\n        midpoint=0.1,\n        labels=lambda breaks: [f\"{x:.1%}\" for x in breaks]\n    ) +\n    \n    # Elegant theming\n    labs(\n        title=\"Academic Program Performance Analysis\",\n        subtitle=\"Bubble size represents sample size, color indicates top performer rate\",\n        x=\"Average Study Hours per Week\",\n        y=\"Average Score (%)\",\n        caption=\"Programs with higher study hours don't always yield higher scores\"\n    ) +\n    theme_minimal() +\n    theme(\n        plot_title=element_text(size=14, weight=\"bold\"),\n        plot_subtitle=element_text(size=11, color=\"#666666\"),\n        legend_position=\"right\",\n        panel_grid_minor=element_blank()\n    )\n)\n\n# Display the plot\np2\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=1000 height=600}\n:::\n:::\n\n\n### 5. Distribution Analysis with Multiple Geometries\n\n::: {#aa725c55 .cell fig-height='8' fig-width='12' execution_count=10}\n``` {.python .cell-code}\n# Focus on advanced courses for distribution analysis\nadvanced_courses = performance_pd[\n    performance_pd[\"course\"].isin([\"Machine Learning\", \"Data Structures\", \"Research Methods\", \"Thesis\"])\n]\n\n# Configure plotnine for this plot\nplotnine.options.figure_size = (12, 8)\n\n# Create comprehensive distribution plot\np3 = (\n    ggplot(advanced_courses, aes(x=\"program\", y=\"score\", fill=\"program\")) +\n    \n    # Violin plots for distribution shape\n    geom_violin(alpha=0.7, trim=False) +\n    \n    # Box plots for summary statistics\n    geom_boxplot(width=0.3, alpha=0.8, outlier_alpha=0.6) +\n    \n    # Add mean points\n    stat_summary(fun_y=np.mean, geom=\"point\", size=3, color=\"white\", shape=\"D\") +\n    \n    # Facet by course\n    facet_wrap(\"course\", ncol=2) +\n    \n    # Color scheme\n    scale_fill_brewer(type=\"qual\", palette=\"Dark2\") +\n    \n    # Coordinate system\n    coord_flip() +\n    \n    # Labels and theme\n    labs(\n        title=\"Score Distribution Analysis for Advanced Courses\",\n        subtitle=\"Violin plots show full distribution, box plots highlight quartiles, diamonds mark means\",\n        x=\"Academic Program\",\n        y=\"Course Score (%)\",\n        fill=\"Program\",\n        caption=\"Advanced courses: Machine Learning, Data Structures, Research Methods, Thesis\"\n    ) +\n    theme_minimal() +\n    theme(\n        plot_title=element_text(size=14, weight=\"bold\"),\n        plot_subtitle=element_text(size=11, color=\"#666666\"),\n        strip_text=element_text(size=11, weight=\"bold\"),\n        legend_position=\"none\",  # Remove legend since x-axis shows programs\n        axis_text_x=element_text(angle=45, hjust=1)\n    )\n)\n\n# Display the plot\n\np3\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=1200 height=800}\n:::\n:::\n\n\n## Performance Comparison: Polars vs Pandas\n\n### 6. Speed Benchmarking\n\n::: {#e4038eef .cell execution_count=11}\n``` {.python .cell-code}\nimport time\nimport pandas as pd\n\n# Create larger dataset for meaningful comparison\nlarge_n = 50000\nlarge_students = pl.DataFrame({\n    \"student_id\": range(1, large_n + 1),\n    \"program\": np.random.choice([\"CS\", \"Math\", \"Physics\", \"Stats\"], large_n),\n    \"score\": np.random.normal(75, 15, large_n),\n    \"study_hours\": np.random.gamma(3, 2, large_n),\n    \"semester\": np.random.choice(range(1, 9), large_n)\n})\n\n# Convert to pandas for comparison\nlarge_students_pd = large_students.to_pandas()\n\nprint(f\"Dataset size: {large_students.shape[0]:,} rows\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset size: 50,000 rows\n```\n:::\n:::\n\n\n::: {#f2383389 .cell execution_count=12}\n``` {.python .cell-code}\n# Benchmark complex aggregation operations\n\ndef benchmark_polars():\n    start_time = time.time()\n    result = (\n        large_students\n        .group_by([\"program\", \"semester\"])\n        .agg([\n            pl.col(\"score\").mean().alias(\"avg_score\"),\n            pl.col(\"score\").std().alias(\"std_score\"),\n            pl.col(\"study_hours\").mean().alias(\"avg_hours\"),\n            pl.col(\"score\").quantile(0.9).alias(\"score_90th\"),\n            pl.count().alias(\"count\")\n        ])\n        .filter(pl.col(\"count\") >= 100)\n        .sort([\"program\", \"semester\"])\n    )\n    end_time = time.time()\n    return end_time - start_time, result.shape[0]\n\ndef benchmark_pandas():\n    start_time = time.time()\n    result = (\n        large_students_pd\n        .groupby([\"program\", \"semester\"])\n        .agg({\n            \"score\": [\"mean\", \"std\", lambda x: x.quantile(0.9)],\n            \"study_hours\": \"mean\",\n            \"student_id\": \"count\"\n        })\n        .reset_index()\n    )\n    # Flatten column names\n    result.columns = [\"_\".join(col).strip() if col[1] else col[0] for col in result.columns]\n    result = result[result.iloc[:, -1] >= 100]  # Filter by count\n    end_time = time.time()\n    return end_time - start_time, result.shape[0]\n\n# Run benchmarks\npolars_time, polars_rows = benchmark_polars()\npandas_time, pandas_rows = benchmark_pandas()\n\nprint(f\"Polars: {polars_time:.4f} seconds ({polars_rows} result rows)\")\nprint(f\"Pandas: {pandas_time:.4f} seconds ({pandas_rows} result rows)\")\nprint(f\"Speedup: {pandas_time/polars_time:.2f}x faster with Polars\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPolars: 0.0016 seconds (32 result rows)\nPandas: 0.0073 seconds (32 result rows)\nSpeedup: 4.69x faster with Polars\n```\n:::\n:::\n\n\n### 7. Memory Usage Analysis\n\n::: {#cd03180a .cell execution_count=13}\n``` {.python .cell-code}\n# Memory usage comparison\nprint(\"Memory usage comparison:\")\nprint(f\"Polars DataFrame: {large_students.estimated_size('mb'):.2f} MB\")\nprint(f\"Pandas DataFrame: {large_students_pd.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n\n# Show data types efficiency\nprint(\"\\nData types:\")\nprint(\"Polars dtypes:\")\nfor col, dtype in zip(large_students.columns, large_students.dtypes):\n    print(f\"  {col}: {dtype}\")\n    \nprint(\"\\nPandas dtypes:\")\nfor col, dtype in large_students_pd.dtypes.items():\n    print(f\"  {col}: {dtype}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMemory usage comparison:\nPolars DataFrame: 1.74 MB\nPandas DataFrame: 4.08 MB\n\nData types:\nPolars dtypes:\n  student_id: Int64\n  program: String\n  score: Float64\n  study_hours: Float64\n  semester: Int64\n\nPandas dtypes:\n  student_id: int64\n  program: object\n  score: float64\n  study_hours: float64\n  semester: int64\n```\n:::\n:::\n\n\n## Advanced plotnine Techniques\n\n### 8. Custom Themes and Statistical Layers\n\n::: {#e13cee93 .cell fig-height='6' fig-width='10' execution_count=14}\n``` {.python .cell-code}\n# Configure plotnine for this plot\nplotnine.options.figure_size = (10, 6)\n\n# Create a custom theme for academic publications\nacademic_theme = theme_minimal() + theme(\n    plot_title=element_text(size=14, weight=\"bold\", margin={\"b\": 20}),\n    plot_subtitle=element_text(size=11, color=\"#4d4d4d\", margin={\"b\": 15}),\n    axis_title=element_text(size=12, weight=\"bold\"),\n    axis_text=element_text(size=10),\n    legend_title=element_text(size=11, weight=\"bold\"),\n    legend_text=element_text(size=10),\n    strip_text=element_text(size=11, weight=\"bold\", margin={\"b\": 10}),\n    panel_grid_major=element_line(color=\"#e6e6e6\", size=0.5),\n    panel_grid_minor=element_blank(),\n    plot_background=element_rect(fill=\"white\"),\n    panel_background=element_rect(fill=\"white\")\n)\n\n# Advanced statistical visualization\nstudy_performance = (\n    performance_df\n    .filter(pl.col(\"course\").is_in([\"Programming\", \"Machine Learning\", \"Statistics\"]))\n    .to_pandas()\n)\n\np4 = (\n    ggplot(study_performance, aes(x=\"attendance\", y=\"score\")) +\n    \n    # Add points with transparency to show density\n    geom_point(aes(color=\"program\"), alpha=0.3, size=0.8) +\n    \n    # Add smooth trend lines\n    geom_smooth(aes(color=\"program\"), method=\"loess\", se=True) +\n    \n    # Facet by course\n    facet_wrap(\"course\", ncol=3) +\n    \n    # Custom color palette\n    scale_color_manual(\n        values=[\"#2E4057\", \"#5D737E\", \"#8FA68E\", \"#C7D59F\"],\n        name=\"Program\"\n    ) +\n    \n    # Coordinate limits\n    xlim(50, 100) +\n    ylim(0, 100) +\n    \n    # Labels\n    labs(\n        title=\"Attendance vs Performance Analysis by Course\",\n        subtitle=\"Point density shows distribution, smooth curves indicate trends\",\n        x=\"Attendance Rate (%)\",\n        y=\"Course Score (%)\",\n        caption=\"Statistical analysis of 40,000 course enrollments\"\n    ) +\n    \n    # Apply custom theme\n    academic_theme +\n    theme(legend_position=\"bottom\")\n)\n\n# Display the plot\ndisplay(p4)\np4\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=1000 height=600}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-2.png){width=1000 height=600}\n:::\n:::\n\n\n## Polars-plotnine Integration Best Practices\n\n### 9. Efficient Data Pipeline\n\n::: {#298d3cfb .cell execution_count=15}\n``` {.python .cell-code}\n# Demonstrate efficient Polars → plotnine workflow\ndef create_analysis_pipeline(data: pl.DataFrame, analysis_type: str):\n    \"\"\"\n    Efficient pipeline that processes data in Polars and visualizes with plotnine\n    \"\"\"\n    \n    if analysis_type == \"performance_trends\":\n        # Complex Polars aggregation\n        processed = (\n            data\n            .with_columns([\n                pl.when(pl.col(\"score\") >= 90).then(pl.lit(\"A\"))\n                  .when(pl.col(\"score\") >= 80).then(pl.lit(\"B\")) \n                  .when(pl.col(\"score\") >= 70).then(pl.lit(\"C\"))\n                  .when(pl.col(\"score\") >= 60).then(pl.lit(\"D\"))\n                  .otherwise(pl.lit(\"F\")).alias(\"grade\")\n            ])\n            .group_by([\"course\", \"program\", \"grade\"])\n            .agg([\n                pl.count().alias(\"student_count\"),\n                pl.col(\"study_hours\").mean().alias(\"avg_study_hours\")\n            ])\n            .with_columns([\n                pl.col(\"student_count\").sum().over([\"course\", \"program\"]).alias(\"total_students\")\n            ])\n            .with_columns([\n                (pl.col(\"student_count\") / pl.col(\"total_students\") * 100).alias(\"percentage\")\n            ])\n            .filter(pl.col(\"total_students\") >= 50)  # Sufficient sample size\n        )\n        \n        # Convert to pandas only for plotting\n        plot_data = processed.to_pandas()\n        \n        # Configure plotnine for this plot\n        plotnine.options.figure_size = (10, 6)\n        \n        # Create visualization\n        p = (\n            ggplot(plot_data, aes(x=\"grade\", y=\"percentage\", fill=\"program\")) +\n            geom_col(position=\"dodge\", alpha=0.8) +\n            facet_wrap(\"course\", ncol=4) +\n            scale_fill_brewer(type=\"qual\", palette=\"Set3\") +\n            labs(\n                title=\"Grade Distribution by Program and Course\",\n                x=\"Grade\", y=\"Percentage of Students (%)\",\n                fill=\"Program\"\n            ) +\n            academic_theme +\n            theme(\n                axis_text_x=element_text(size=12, weight=\"bold\"),\n                legend_position=\"bottom\"\n            )\n        )\n        \n        return processed, p\n    \n    else:\n        raise ValueError(\"Unknown analysis type\")\n\n# Execute pipeline\ngrade_analysis, grade_plot = create_analysis_pipeline(performance_df, \"performance_trends\")\n\nprint(\"Processed data shape:\", grade_analysis.shape)\n# Display the plot\ndisplay(grade_plot)\ngrade_plot\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessed data shape: (139, 7)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-2.png){width=1000 height=600}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-3.png){width=1000 height=600}\n:::\n:::\n\n\n## Real-World Applications\n\n### 10. Educational Data Science Workflow\n\n::: {#cf6e0f6e .cell execution_count=16}\n``` {.python .cell-code}\n# Simulate a complete educational analytics workflow\n\n# 1. Data Quality Assessment with Polars\n# Create quality report with separate operations to avoid mixing agg types\nnull_counts = performance_df.null_count()\nstats_summary = performance_df.select([\n    pl.col(\"score\").min().alias(\"score_min\"),\n    pl.col(\"score\").max().alias(\"score_max\"),\n    pl.col(\"score\").mean().alias(\"score_mean\"),\n])\nquality_flags = performance_df.select([\n    (pl.col(\"score\") < 0).sum().alias(\"negative_scores\"),\n    (pl.col(\"score\") > 100).sum().alias(\"invalid_scores\"),\n    (pl.col(\"study_hours\") < 0).sum().alias(\"negative_hours\"),\n])\n\nprint(\"Data Quality Report:\")\nprint(\"Null counts:\")\nprint(null_counts)\nprint(\"\\nStatistical summary:\")\nprint(stats_summary)\nprint(\"\\nQuality flags:\")\nprint(quality_flags)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData Quality Report:\nNull counts:\nshape: (1, 8)\n┌────────────┬─────────┬─────┬────────┬──────────┬───────┬─────────────┬────────────┐\n│ student_id ┆ program ┆ gpa ┆ course ┆ semester ┆ score ┆ study_hours ┆ attendance │\n│ ---        ┆ ---     ┆ --- ┆ ---    ┆ ---      ┆ ---   ┆ ---         ┆ ---        │\n│ u32        ┆ u32     ┆ u32 ┆ u32    ┆ u32      ┆ u32   ┆ u32         ┆ u32        │\n╞════════════╪═════════╪═════╪════════╪══════════╪═══════╪═════════════╪════════════╡\n│ 0          ┆ 0       ┆ 0   ┆ 0      ┆ 0        ┆ 0     ┆ 0           ┆ 0          │\n└────────────┴─────────┴─────┴────────┴──────────┴───────┴─────────────┴────────────┘\n\nStatistical summary:\nshape: (1, 3)\n┌───────────┬───────────┬────────────┐\n│ score_min ┆ score_max ┆ score_mean │\n│ ---       ┆ ---       ┆ ---        │\n│ f64       ┆ f64       ┆ f64        │\n╞═══════════╪═══════════╪════════════╡\n│ 15.3      ┆ 98.0      ┆ 67.94946   │\n└───────────┴───────────┴────────────┘\n\nQuality flags:\nshape: (1, 3)\n┌─────────────────┬────────────────┬────────────────┐\n│ negative_scores ┆ invalid_scores ┆ negative_hours │\n│ ---             ┆ ---            ┆ ---            │\n│ u32             ┆ u32            ┆ u32            │\n╞═════════════════╪════════════════╪════════════════╡\n│ 0               ┆ 0              ┆ 0              │\n└─────────────────┴────────────────┴────────────────┘\n```\n:::\n:::\n\n\n::: {#e2da5b54 .cell execution_count=17}\n``` {.python .cell-code}\n# 2. Predictive modeling preparation\n# Check what columns we have available\nprint(\"Performance DataFrame columns:\", performance_df.columns)\n\n# Create modeling features directly from performance_df (which already includes key student data)\nmodeling_data = (\n    performance_df\n    .with_columns([\n        # Feature engineering - simplified approach\n        pl.col(\"score\").shift(1, fill_value=0).over(\"student_id\").alias(\"previous_score\"),\n        pl.col(\"study_hours\").mean().over(\"student_id\").alias(\"avg_study_hours_student\"),\n        (pl.col(\"attendance\") >= 85).alias(\"high_attendance\"),\n        \n        # Target encoding - course difficulty (average score for each course)\n        pl.col(\"score\").mean().over(\"course\").alias(\"course_difficulty\"),\n        \n        # Interaction features\n        (pl.col(\"study_hours\") * pl.col(\"attendance\") / 100.0).alias(\"effective_study_time\"),\n        \n        # Course progress indicator\n        pl.col(\"semester\").rank().over(\"student_id\").alias(\"course_sequence\")\n    ])\n    .filter(pl.col(\"score\").is_not_null())  # Remove missing values for modeling\n)\n\nprint(\"Modeling dataset shape:\", modeling_data.shape)\nprint(\"Features available for modeling:\")\nprint(modeling_data.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPerformance DataFrame columns: ['student_id', 'program', 'gpa', 'course', 'semester', 'score', 'study_hours', 'attendance']\nModeling dataset shape: (40000, 14)\nFeatures available for modeling:\n['student_id', 'program', 'gpa', 'course', 'semester', 'score', 'study_hours', 'attendance', 'previous_score', 'avg_study_hours_student', 'high_attendance', 'course_difficulty', 'effective_study_time', 'course_sequence']\n```\n:::\n:::\n\n\n::: {#2a03702f .cell fig-height='6' fig-width='12' execution_count=18}\n``` {.python .cell-code}\n# 3. Final comprehensive visualization\nfinal_plot_data = modeling_data.to_pandas()\n\n# Configure plotnine for this plot\nplotnine.options.figure_size = (12, 6)\n\np_final = (\n    ggplot(final_plot_data.sample(2000), aes(x=\"effective_study_time\", y=\"score\")) +\n    \n    # Use points with alpha for density visualization\n    geom_point(aes(color=\"program\"), alpha=0.4, size=1.5) +\n    \n    # Overlay trend line\n    geom_smooth(color=\"red\", method=\"loess\") +\n    \n    # Facet by program\n    facet_wrap(\"program\", ncol=2) +\n    \n    # Color scale for points\n    scale_color_brewer(type=\"qual\", palette=\"Set2\", name=\"Program\") +\n    \n    # Labels\n    labs(\n        title=\"Effective Study Time vs Academic Performance\",\n        subtitle=\"Point density shows student distribution, red line indicates trend\",\n        x=\"Effective Study Time (hours × attendance rate)\",\n        y=\"Course Score (%)\",\n        caption=\"Sample of 2,000 students from modeling dataset\"\n    ) +\n    \n    # Professional theme\n    academic_theme +\n    theme(\n        strip_text=element_text(size=12, weight=\"bold\"),\n        legend_position=\"right\"\n    )\n)\n\n# Display the plot\ndisplay(p_final)\np_final\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=1000 height=600}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-2.png){width=1000 height=600}\n:::\n:::\n\n\n## Key Takeaways and Best Practices\n\n### Performance Benefits\n1. **Polars advantages**: 2-10x faster than pandas for most operations\n2. **Memory efficiency**: Lower memory footprint with optimized data types\n3. **Lazy evaluation**: Query optimization before execution\n4. **Parallel processing**: Automatic multi-threading\n\n### Visualization Excellence  \n1. **Grammar of graphics**: Systematic approach to building complex visualizations\n2. **Layer composition**: Build plots incrementally for clarity\n3. **Consistent aesthetics**: Professional appearance with minimal code\n4. **Cross-platform**: Same syntax as R's ggplot2\n\n### Integration Strategy\n1. **Data processing in Polars**: Leverage speed for heavy computations\n2. **Visualization in plotnine**: Convert to pandas only when plotting\n3. **Memory management**: Process in chunks for very large datasets\n4. **Type consistency**: Ensure proper data types throughout pipeline\n\n### Educational Applications\n- **Performance analytics**: Fast processing of large student datasets\n- **Interactive exploration**: Quick iteration during analysis\n- **Publication-ready plots**: Professional visualizations for research\n- **Reproducible workflows**: Clear, readable data science pipelines\n\nThe combination of Polars and plotnine represents the future of Python data science: blazing-fast processing with elegant, declarative visualization. This powerful duo enables researchers and educators to handle larger datasets while creating more sophisticated analyses and beautiful visualizations.\n\n## Conclusion\n\nPolars and plotnine together offer a compelling alternative to the traditional pandas + matplotlib ecosystem:\n\n- **Polars** delivers exceptional performance for data manipulation with an intuitive API\n- **plotnine** provides the grammar of graphics for systematic visualization\n- **Together** they enable fast, elegant, and reproducible data science workflows\n\nFor educational data analysis, this combination is particularly powerful, allowing researchers to:\n- Process large institutional datasets efficiently\n- Create publication-quality visualizations\n- Build reproducible analytical pipelines  \n- Scale analyses as data grows\n\nThe investment in learning these tools pays dividends in both performance and code clarity, making them excellent choices for modern Python data science.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}